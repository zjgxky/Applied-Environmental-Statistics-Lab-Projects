---
title: 'Lab 08: Heteroskedasticity'
author: "EE509"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives

In this lab we're going to:

* Explore putting process models on variances (heteroskedasticity)
* Explore Bayesian model selection

# Tasks

### Load & Plot Data

```{r}
load("data/Lab08_het.RData")
plot(x,y)
```

# Fit traditional linear model

Start from the basic linear model from lab 5. Fit the model to the data, perform you standard set of Bayesian metrics [trace plots, densities, GBR, pairs, effective sample size, etc.], and plot the resulting model (CI and PI) and data. When simulating your PI, make sure you've got the residual error in the right units (precision vs SD)

```{r}
library(rjags)
library(coda)

model_homo <- "
model{

  b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
  S ~ dgamma(s1,s2)    ## prior precision

  for(i in 1:n){
	  mu[i] <- b[1] + b[2]*x[i]   	## process model
	  y[i]  ~ dnorm(mu[i],S)		        ## data model
	  like[i] <- dnorm(y[i],mu[i],S)
  }
}
"

data <- list(x = x, y = y, n = length(y))

## specify priors
data$b0 <- as.vector(c(0,0))      ## regression b means
# make a diagonal matrix of size 2*2, SD = 100, precision = 1/10000
data$Vb <- solve(diag(10000,2))   ## regression b precisions
data$s1 <- 0.1                    ## error prior n/2
data$s2 <- 0.1                    ## error prior SS/2

j.model   <- jags.model (file = textConnection(model_homo),
                             data = data,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","S","like"),
                                n.iter = 5000)

codaSplit <- function(jags.out,pattern){
  out = list()
  mfit = as.matrix(jags.out,chains=TRUE)
  pat.cols = grep(pattern,colnames(mfit),fixed=TRUE)
  chain.col = which(colnames(mfit)=="CHAIN")
  out[[1]] = mat2mcmc.list(mfit[,c(chain.col,pat.cols)])
  out[[2]]   = mat2mcmc.list(mfit[,-pat.cols])
  return(out)
}

mat2mcmc.list <- function(w) {
  temp <- list()
  chain.col <- which(colnames(w) == "CHAIN")
  for (i in unique(w[, "CHAIN"])) {
    temp[[i]] <- coda:::as.mcmc(w[w[, "CHAIN"] == i, -chain.col])
  }
  return(as.mcmc.list(temp))
}

jout <- codaSplit(jags.out,"like")
plot(jout[[2]])
gelman.diag(jout[[2]])
GBR <- gelman.plot(jout[[2]])

## burn-in
burnin = 500                                ## determine convergence, choose 500 from GBR

jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
jout <- codaSplit(jags.burn,"like")
plot(jout[[2]])                             ## check diagnostics post burn-in
effectiveSize(jout[[2]])                    ## effective size

out <- as.matrix(jout[[2]])
## Pairwise scatter plots & correlation
pairs(out) ## pairs plot to evaluate parameter correlation
cor(out)    ## correlation matrix among model parameters, b1 and b2 have negative correlation

## credible and prediction intervals
n = nrow(out)
xpred <- seq(0,10,length=30)
npred <- length(xpred)
ypred <- matrix(NA,nrow=n,ncol=npred)
ycred <- matrix(NA,nrow=n,ncol=npred)

for(g in 1:n){
  Ey <- out[g,"b[1]"] + out[g,"b[2]"] * xpred ## expected value of y
  ycred[g,] <- Ey
  ypred[g,] <- rnorm(npred,Ey,sqrt(1/out[g,"S"]))
}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))
pi <- apply(ypred,2,quantile,c(0.025,0.975))

plot(x,y)
lines(xpred,ci[2,],col=2,lwd=2)  ## median model
lines(xpred,ci[1,],col=2,lty=1) ## model CI
lines(xpred,ci[3,],col=2,lty=1)
lines(xpred,pi[1,],col=3,lty=2) ## model PI
lines(xpred,pi[2,],col=3,lty=2)
```

## Calculate model selection metrics

### DIC

```{r}
DIC.ho <- dic.samples(j.model, n.iter=5000)
DIC.ho
DIC1 <- sum(DIC.ho$dev + DIC.ho$pen)
DIC1
```

### WAIC

First, within you JAGS model, add the likelihood calculation within your for loop
```
 like[i] <- dnorm(y[i],mu[i],S)
```
Second, assuming that you've converted your JAGS output to a matrix to make the pairs plots and other diagnostics (e.g. `out <- as.matrix(jags.burn)`) we'll want to grab those likelihood columns to calculate WAIC. We'll do that using the `grepl` pattern matching function and the regular expression character `^` which tells R to find any column names that start with the following characters (in this case `like`). Once we do that we'll follow the same calculation as in the  

```{r}
   out = as.matrix(jout[[1]])
   like   <- out[,grepl("^like",colnames(out))] 
   fbar   <- colMeans(like)
   Pw     <- sum(apply(log(like),2,var))
   WAIC.ho   <- -2*sum(log(fbar))+2*Pw
   WAIC.ho
```
You'll also notice that out output now has a lot of `like` columns that complicate a lot of our other `coda` diagnostics. We can also use `grepl` to _exclude_ all the columns that have a pattern. For example:
```{r}
# pairs(out[,!grepl("^like",colnames(out))])
```

### Predictive loss

The code for predictive loss is very similar to our code for generating confidence and predictive intervals, with the biggest different being that the calculations are done at the OBSERVED X's not a sequence of X's (though if you sort your X's you can often use that sequence to draw the CI & PI). 
```{r}
ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
  ycred[g,] <- out[g,2] + out[g,3] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1]))
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.ho <- c(G,P,Dpl)
PL.ho
```
Note: for these metrics I've added `.ho` onto the end of the name for the homoskedastic model. For the heterskedastic model you'll want to change this to something different (e.g. `.he`) so that you don't overwrite the results from your first models (you'll need both to make the table at the end)

# Fit heteroskedastic model 

To add heteroskedasticity, we'll start with the linear regression model and then modify it as follows:

* Within the JAGS `for` loop, add a process model for the calculation of the precision

```
  s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
  S[i] <- 1/s[i]^2          ## calculate precision from SD
```

```{r}
model_hetero <- "
model{

  b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
  a1 ~ dexp(1)
  a2 ~ dlnorm(atwo,Vatwo)

  for(i in 1:n){
	  mu[i] <- b[1] + b[2]*x[i]   	## process model
	  s[i] <- a1 + a2*x[i]  ## linear model on standard deviation
	  S[i] <- 1/s[i]^2
	  y[i]  ~ dnorm(mu[i],S[i])		     ## data model
	  like[i] <- dnorm(y[i],mu[i],S[i])
  }
}
"

data <- list(x = x, y = y, n = length(y))

## specify priors
data$b0 <- as.vector(c(0,0))      ## regression b means
# make a diagonal matrix of size 2*2, SD = 100, precision = 1/10000
data$Vb <- solve(diag(10000,2))   ## regression b precisions
data$atwo <- as.vector(c(0))      ## error prior n/2
data$Vatwo <- solve(10000)   ## error prior SS/2

j.model   <- jags.model (file = textConnection(model_hetero),
                             data = data,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","a1","a2","like"),
                                n.iter = 5000)
```

```{r}
codaSplit <- function(jags.out,pattern){
  out = list()
  mfit = as.matrix(jags.out,chains=TRUE)
  pat.cols = grep(pattern,colnames(mfit),fixed=TRUE)
  chain.col = which(colnames(mfit)=="CHAIN")
  out[[1]] = mat2mcmc.list(mfit[,c(chain.col,pat.cols)])
  out[[2]]   = mat2mcmc.list(mfit[,-pat.cols])
  return(out)
}

mat2mcmc.list <- function(w) {
  temp <- list()
  chain.col <- which(colnames(w) == "CHAIN")
  for (i in unique(w[, "CHAIN"])) {
    temp[[i]] <- coda:::as.mcmc(w[w[, "CHAIN"] == i, -chain.col])
  }
  return(as.mcmc.list(temp))
}

jout <- codaSplit(jags.out,"like")
plot(jout[[2]])
gelman.diag(jout[[2]])
GBR <- gelman.plot(jout[[2]])

## burn-in
burnin = 500                                ## determine convergence
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
jout <- codaSplit(jags.burn,"like")
plot(jout[[2]])                             ## check diagnostics post burn-in
effectiveSize(jout[[2]])                    ## effective size

## Pairwise scatter plots & correlation
out = as.matrix(jout[[2]])
pairs(out)	## pairs plot to evaluate parameter correlation
cor(out)    ## correlation matrix among model parameters

out <- as.matrix(jags.out)
n = nrow(out)
xpred <- seq(0,10,length=30)
npred <- length(xpred)
ypred <- matrix(NA,nrow=n,ncol=npred)
ycred <- matrix(NA,nrow=n,ncol=npred)

for(g in 1:n){
  Ey <- out[g,"b[1]"] + out[g,"b[2]"] * xpred ## expected value of y
  ycred[g,] <- Ey
  ypred[g,] <- rnorm(npred,Ey,sqrt(1/out[g,"a1"] + 1/out[g,"a2"] * xpred))
}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))
pi <- apply(ypred,2,quantile,c(0.025,0.975))

plot(x,y)
lines(xpred,ci[2,],col=2,lwd=2)  ## median model
lines(xpred,ci[1,],col=2,lty=1) ## model CI
lines(xpred,ci[3,],col=2,lty=1)
lines(xpred,pi[1,],col=3,lty=2) ## model PI
lines(xpred,pi[2,],col=3,lty=2)
```

```{r}
DIC.he <- dic.samples(j.model, n.iter=5000)
DIC.he
DIC2 <- sum(DIC.he$dev + DIC.he$pen)
DIC2

out = as.matrix(jout[[1]])
like   <- out[,grepl("^like",colnames(out))] 
fbar   <- colMeans(like)
Pw     <- sum(apply(log(like),2,var))
WAIC.he   <- -2*sum(log(fbar))+2*Pw
WAIC.he

ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
  ycred[g,] <- out[g,3] + out[g,4] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1] + 1/out[g,2] * xpred))
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.he <- c(G,P,Dpl)
PL.he
```

```{r}
tab <- matrix(c(DIC1, WAIC.ho, PL.ho[3], DIC2, WAIC.he, PL.he[3]), ncol=3, byrow=TRUE)
colnames(tab) <- c('DIC','WAIC', 'PL')
rownames(tab) <- c('homo','hetero')
tab <- as.table(tab)
tab
# Heteroskedastic model is better as DIC, WAIC, PL are all smaller compared to homoskedastic model. That is because heteroskedastic model has lower residual errors. Therefore, for Predictive Loss, decrease in G overweights the increase in P in heteroskedastic model. For DIC, heteroskedastic model also has much less mean deviance although its penalty is a little bit higher.
```


* Replace prior on `S` with priors on `a[1]` and `a[2]`. To ensure that our variance is always positive, make sure to choose zero-bound prior distributions on `a`. Don't forget to add any new prior parameters to your `data` list.

* Update data model and WAIC likelihood calculation to use `S[i]` instead of a fixed `S`.

* Update your `coda.samples` to include `a` instead of `S`.

* As before, perform your standard MCMC metrics & diagnostics

* Calculate your three model selection metrics (DIC, WAIC, PL)
  ** For predictive loss, CI, and PI, don't forget to update your process model to include the process model on sigma and to make sure you're grabbing the right parameters! And don't forget the precision vs SD difference between R and JAGS.

* Plot your model and data with CI and PI

* As a final task, make a table that shows the different model selection metrics for both models. Briefly discuss how the metrics performed, what they told us, and where they are the same or different.


